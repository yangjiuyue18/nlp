{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение векторных моделей слов**:\n",
    "   - Модели **Word2Vec** и **Doc2Vec** были обучены с помощью библиотеки `gensim` для извлечения семантической информации из текста, соответственно.\n",
    "   - В методе `Word2Vec` документ представляется путем усреднения векторов всех слов в документе.\n",
    "   - В методе `Doc2Vec` векторы уровня документа генерируются напрямую.\n",
    "\n",
    "## Экспериментальные результаты\n",
    "\n",
    "- При представлении документов с помощью Word2Vec точность классификации составляет 70 %, а результат F1 - 69 %.\n",
    "- При использовании Doc2Vec для представления документов точность классификации повышается до 79 %, а показатель F1 - до 79 %, особенно для небольших категорий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Models: word2vec.model\n",
      "Train the classifier...\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.69      0.11      0.20        79\n",
      "     culture       0.69      0.68      0.68       279\n",
      "   economics       0.66      0.86      0.74       266\n",
      "      forces       0.57      0.70      0.63       149\n",
      "        life       0.66      0.72      0.69       288\n",
      "       media       0.69      0.62      0.65       299\n",
      "     science       0.75      0.73      0.74       288\n",
      "       sport       0.89      0.89      0.89       276\n",
      "       style       0.70      0.55      0.62        38\n",
      "      travel       0.30      0.08      0.12        38\n",
      "\n",
      "    accuracy                           0.70      2000\n",
      "   macro avg       0.66      0.59      0.60      2000\n",
      "weighted avg       0.70      0.70      0.69      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def train_or_load_model(data_path, model_path=\"word2vec.model\"):\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading Models: {model_path}\")\n",
    "        return Word2Vec.load(model_path)\n",
    "    \n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    sentences = [word_tokenize(line.split('\\t')[2].lower()) for line in lines if '\\t' in line]\n",
    "\n",
    "    print(\"Training a new Word2Vec model...\")\n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved: {model_path}\")\n",
    "    return model\n",
    "\n",
    "def document_to_vector(doc, model):\n",
    "\n",
    "    words = word_tokenize(doc.lower())\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if not vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "def classify_texts(data_path, model):\n",
    "\n",
    "    stop_words = set(stopwords.words(\"russian\"))\n",
    "\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    labels, texts = zip(\n",
    "        *[(line.split('\\t')[0], line.split('\\t')[2]) for line in lines if '\\t' in line]\n",
    "    )\n",
    "    \n",
    "    vectors = np.array([\n",
    "        document_to_vector(\n",
    "            \" \".join(word for word in word_tokenize(text.lower()) if word not in stop_words), \n",
    "            model\n",
    "        )\n",
    "        for text in texts\n",
    "    ])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(vectors, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    classifier = SVC(kernel='linear')\n",
    "    print(\"Train the classifier...\")\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = '../data/news.txt'\n",
    "    model_path = 'word2vec.model'\n",
    "\n",
    "    model = train_or_load_model(data_path, model_path)\n",
    "    classify_texts(data_path, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Models: doc2vec.model\n",
      "Train the classifier...\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.57      0.42      0.48        79\n",
      "     culture       0.85      0.79      0.82       279\n",
      "   economics       0.78      0.87      0.82       266\n",
      "      forces       0.76      0.73      0.74       149\n",
      "        life       0.72      0.80      0.75       288\n",
      "       media       0.77      0.72      0.75       299\n",
      "     science       0.81      0.86      0.83       288\n",
      "       sport       0.92      0.93      0.92       276\n",
      "       style       0.92      0.63      0.75        38\n",
      "      travel       0.64      0.47      0.55        38\n",
      "\n",
      "    accuracy                           0.79      2000\n",
      "   macro avg       0.77      0.72      0.74      2000\n",
      "weighted avg       0.79      0.79      0.79      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "def prepare_tagged_documents(data_path):\n",
    "\n",
    "    documents = []\n",
    "    labels = []\n",
    "    stop_words = set(stopwords.words(\"russian\"))\n",
    "\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 3:\n",
    "                label, _, text = parts\n",
    "                tokens = word_tokenize(text.lower())\n",
    "                filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "                documents.append(TaggedDocument(words=filtered_tokens, tags=[f'doc_{i}']))\n",
    "                labels.append(label)\n",
    "    return documents, labels\n",
    "\n",
    "def train_or_load_doc2vec(documents, model_path=\"doc2vec.model\", vector_size=100):\n",
    "\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading Models: {model_path}\")\n",
    "        model = Doc2Vec.load(model_path)\n",
    "    else:\n",
    "        print(\"Training a new Doc2Vec model...\")\n",
    "        model = Doc2Vec(\n",
    "            documents,\n",
    "            vector_size=vector_size,\n",
    "            window=5,\n",
    "            min_count=2,\n",
    "            workers=4,\n",
    "            epochs=20,\n",
    "            dm=1  # PV-DM model\n",
    "        )\n",
    "        model.save(model_path)\n",
    "        print(f\"Model saved: {model_path}\")\n",
    "    return model\n",
    "\n",
    "def get_document_vectors(model, documents):\n",
    "\n",
    "    vectors = []\n",
    "    for doc in documents:\n",
    "        vector = model.infer_vector(doc.words)\n",
    "        vectors.append(vector)\n",
    "    return np.array(vectors)\n",
    "\n",
    "def classify_texts_with_doc2vec(data_path, model_path=\"doc2vec.model\"):\n",
    "\n",
    "    documents, labels = prepare_tagged_documents(data_path)\n",
    "    \n",
    "    train_docs, test_docs, train_labels, test_labels = train_test_split(\n",
    "        documents, labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    model = train_or_load_doc2vec(train_docs, model_path=model_path)\n",
    "    \n",
    "    X_train = get_document_vectors(model, train_docs)\n",
    "    X_test = get_document_vectors(model, test_docs)\n",
    "    \n",
    "    print(\"Train the classifier...\")\n",
    "    classifier = SVC(kernel='rbf', C=10)\n",
    "    classifier.fit(X_train, train_labels)\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(test_labels, y_pred))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = \"../data/news.txt\"\n",
    "    model_path = \"doc2vec.model\"\n",
    "    classify_texts_with_doc2vec(data_path, model_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
