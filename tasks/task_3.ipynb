{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "!pip install sentence-transformers\n",
    "!pip install numpy\n",
    "!pip install tensorflow==2.12.0 keras==2.12.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete, saved to: data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def preprocess_news(file_path, output_path):\n",
    "    \"\"\"\n",
    "    Предварительно обрабатывает новостные данные и преобразует их в формат JSON.\n",
    "    :param file_path: Путь сырых новостных данных\n",
    "    :param output_path: Путь к обработанному файлу JSON\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 3:\n",
    "                label, title, content = parts\n",
    "                combined_text = f\"{label}: {title}. {content}\"\n",
    "                data[str(i)] = combined_text\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(data, outfile, ensure_ascii=False, indent=4)\n",
    "    print(f\"Preprocessing complete, saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = '../data/news.txt'\n",
    "    output_path = 'data.json'\n",
    "    preprocess_news(file_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start generating vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 313/313 [01:23<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index construction is complete with 10000 vectors\n",
      "Indexes and metadata are saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "\n",
    "def build_index(data_path, model_name, index_path):\n",
    "    \"\"\"\n",
    "    Построение индексов семантического поиска.\n",
    "    :param data_path: Путь к новостным данным в формате JSON\n",
    "    :param model_name: Hugging Face Название модели\n",
    "    :param index_path: Путь сохранения индекса\n",
    "    \"\"\"\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    texts = list(data.values())\n",
    "    ids = list(data.keys())\n",
    "    \n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(\"Start generating vectors...\")\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])  #  L2 расстояние\n",
    "    index.add(np.array(embeddings))\n",
    "    print(f\"Index construction is complete with {index.ntotal} vectors\")\n",
    "\n",
    "    faiss.write_index(index, index_path)\n",
    "    with open(\"metadata.json\", \"w\", encoding=\"utf-8\") as meta_file:\n",
    "        json.dump(ids, meta_file)\n",
    "    print(\"Indexes and metadata are saved\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = 'data.json'\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    index_path = \"vector_index.faiss\"\n",
    "    build_index(data_path, model_name, index_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: Исторические события в Китае\n",
      "Результаты поиска:\n",
      "ID: 9100, Distance: 0.8356\n",
      "ID: 3995, Distance: 0.8398\n",
      "ID: 1416, Distance: 0.8679\n",
      "ID: 2231, Distance: 0.8701\n",
      "ID: 6690, Distance: 0.8715\n",
      "ID: 1862, Distance: 0.8733\n",
      "ID: 874, Distance: 0.8886\n",
      "ID: 676, Distance: 0.8891\n",
      "ID: 8272, Distance: 0.8958\n",
      "ID: 4923, Distance: 0.8976\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "\n",
    "def search_query(query, model_name, index_path, meta_path, distance_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Семантический поиск с фильтрацией по порогу схожести.\n",
    "    :param query: Текст запроса\n",
    "    :param model_name: Hugging Face Название модели\n",
    "    :param index_path: Путь к индексному файлу\n",
    "    :param meta_path: Путь к файлу метаданных\n",
    "    :param distance_threshold: Порог расстояния для фильтрации\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    query_vec = model.encode([query])\n",
    "    \n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as meta_file:\n",
    "        metadata = json.load(meta_file)\n",
    "    \n",
    "    distances, indices = index.search(np.array(query_vec), index.ntotal)\n",
    "    results = [\n",
    "        {\"id\": metadata[i], \"distance\": distances[0][idx]}\n",
    "        for idx, i in enumerate(indices[0])\n",
    "        if distances[0][idx] < distance_threshold\n",
    "    ]\n",
    "    \n",
    "    print(f\"query: {query}\")\n",
    "    if results:\n",
    "        print(\"Search results:\")\n",
    "        for result in results:\n",
    "            print(f\"ID: {result['id']}, Distance: {result['distance']:.4f}\")\n",
    "    else:\n",
    "        print(\"No matching results were found\")\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter request text: \")\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    index_path = \"vector_index.faiss\"\n",
    "    meta_path = \"metadata.json\"\n",
    "    \n",
    "    search_query(query, model_name, index_path, meta_path, distance_threshold=0.9)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
